{"cells":[{"cell_type":"markdown","source":["#Klasyfikacja Gaussowskim i wielomianowym klasyfikatorami Bayesowskimi i SVC opartymi o reprezentacje zliczające tokeny (Baf of Words i Bag of n-grams)"],"metadata":{"id":"lTM3qgO0-B92"}},{"cell_type":"markdown","source":["Ten plik pozwala na seryjne testowanie klasyfikatorów i pozyskanie wyników (w formacie tabeli latexowych i macierzy pomyłek) dla określonych zbiorów zapisanych w formacie pickle. Uruchomienie wszsytkich komórek z notatnika powoduje wygenerowanie wyników dla wszystkich okreśonych zbiorów zapamiętanych w formacie pickle."],"metadata":{"id":"uBmpnx2v-Rm-"}},{"cell_type":"markdown","source":["### Działania przygotowawcze: podłączenie Dysku Google, instalacja wymaganych pakietów, dołączenie katalogu projektu do ścieżki systemowej i import potrzebnych klas z projektu"],"metadata":{"id":"gcJ9Vaif-U-P"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"z3y-Cr0OfmOr"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd gdrive/MyDrive/authorship_anaysis/authorship_analysis/training_models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DxIQyDq8fnJX"},"outputs":[],"source":["! pip install -r ../requrements.txt\n","! pip install https://github.com/kpu/kenlm/archive/master.zip\n","! python -m spacy download pl_core_news_lg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUoxUsQne5ok"},"outputs":[],"source":["import os\n","import sys\n","module_path = os.path.abspath(os.path.join('..'))\n","if module_path not in sys.path:\n","    sys.path.append(module_path)\n","print(module_path)\n","print(sys.path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OYqTghXWe5oo"},"outputs":[],"source":["from data_preparation import CorpusPreparation\n","from pipelines import Pipeline, Explain\n","from data_fetchers import EpochsFetcher, BookSet"]},{"cell_type":"markdown","source":["### Zbiory treningowe i testowe"],"metadata":{"id":"PkT1YFjw_n4F"}},{"cell_type":"markdown","source":["Lista dostępnych zbiorów zapisanych jako pickle. Zbiory oznaczone są liczbami naturalnymi i cyfra dziesiątek oznacza sposób podziału zbioru na zbiory testowe i uczące z określoną liczbą słów w próbce, a cyfra jednosci oznacza wybrany preprocessing."],"metadata":{"id":"ngSlpcUj_rvT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yt7JMm5NiGVI"},"outputs":[],"source":["import pickle\n","from os.path import exists\n","datasets_filepath = 'datasets/features_'\n","for i in range(100):\n","  path = datasets_filepath + str(i)\n","  if exists(path):\n","    print(path)\n","    with open(path, 'rb') as f:\n","      data = pickle.load(f)\n","      head_data, _, _, _, _ = data\n","      print(f'test_size: {head_data[0]}, train_size: {head_data[1]}, number of words in paragraph: {head_data[2]}, preprocessing operations: {head_data[3]}, authors: {head_data[4]}')"]},{"cell_type":"markdown","source":["### Określenie rodzajów preprocessingu i wybór zbiorów, deklaracja zmiennych"],"metadata":{"id":"_aEdbggOAqxz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MymqlvwAe5oq"},"outputs":[],"source":["preps_num = [0,1,2,3,5] # wybór preprocessingu\n","preps_descr = ['a', 'als', 'alo_NVAdj', 'alp', 'alsp', 'alomp_NVAdj']\n","div_num= [5,7]#[0,1,2,3,4,5,6,7] # wybór numeru podziału zbioru z wielkością próbki\n","number_of_dataset = 0\n","\n","mixed = None\n","test_size = None\n","train_size = None\n","words_num_in_par = None\n","representations = ['bo3'] # 'bow', 'bo2', 'bo3'\n","models= ['mnb'] # 'cnb, 'svc', gnb'\n","authors = []\n","preprocessing_list = []\n","train_classes = []\n","prep_descr = ''\n","kinds_descr = ''\n","latex_set_str = ''\n"]},{"cell_type":"markdown","source":["### Definicje funkcji"],"metadata":{"id":"Q8Ga6lRFAtqD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vSiF_WqRcMV-"},"outputs":[],"source":["def get_books_epoch_list():\n","  print(authors)\n","  bookset = BookSet()\n","  bookset.fetch()\n","  if authors == ['Eliza Orzeszkowa', 'Henryk Sienkiewicz', 'Bolesław Prus']:\n","    kinds =['Epika']\n","    books_list = bookset.list_of_books_to_author_title_list(bookset.get_books_by_authors_list_kinds(authors, kinds))\n","    books_epoch_list = [books_list]\n","  if authors == ['Daniel Naborowski', 'Mikołaj Sęp Szarzyński', 'Elżbieta Drużbacka', 'Adam Mickiewicz', 'Juliusz Słowacki', 'Cyprian Kamil Norwid', 'Adam Asnyk', 'Maria Konopnicka', 'Kazimierz Przerwa-Tetmajer', 'Bolesław Leśmian', 'Jan Kasprowicz']:\n","    kinds = ['Liryka']\n","    authors_b = ['Daniel Naborowski', 'Mikołaj Sęp Szarzyński', 'Elżbieta Drużbacka']\n","    authors_r = ['Adam Mickiewicz', 'Juliusz Słowacki', 'Cyprian Kamil Norwid']\n","    authors_p = ['Adam Asnyk', 'Maria Konopnicka']\n","    authors_m = ['Kazimierz Przerwa-Tetmajer', 'Bolesław Leśmian', 'Jan Kasprowicz']\n","    books_list_b = bookset.list_of_books_to_author_title_list(bookset.get_books_by_authors_list_kinds(authors_b, kinds))\n","    books_list_r = bookset.list_of_books_to_author_title_list(bookset.get_books_by_authors_list_kinds(authors_r, kinds))\n","    books_list_p = bookset.list_of_books_to_author_title_list(bookset.get_books_by_authors_list_kinds(authors_p, kinds))\n","    books_list_m = bookset.list_of_books_to_author_title_list(bookset.get_books_by_authors_list_kinds(authors_m, kinds))\n","    books_epoch_list = [books_list_b, books_list_r, books_list_p, books_list_m]\n","  return books_epoch_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-QMNeeQe5or"},"outputs":[],"source":["def get_existing_authors():\n","  existing_authors_set = set(train_classes)\n","  #print(existing_authors_set)\n","  authors_max_books = {}\n","  authors_ = []\n","  books_epoch_list = get_books_epoch_list()\n","  #print(books_epoch_list)\n","  for books_epoch in books_epoch_list:\n","      authors_list = [author for (author, title) in books_epoch]\n","      authors_set = sorted(list(set(authors_list)))\n","      for author in authors_set:\n","          if author in existing_authors_set:\n","              books_num = authors_list.count(author)\n","              if author in authors_:\n","                  if books_num > authors_max_books[author]:\n","                      authors_.remove(author)\n","                      authors_max_books[author] = books_num\n","                      authors_.append(author)\n","              else:\n","                  authors_.append(author)\n","                  authors_max_books[author] = books_num\n","  return authors_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qgMfrFz7Ty4o"},"outputs":[],"source":["def conf_path():\n","  div = ''\n","  if not mixed:\n","      div = '_div'\n","  path = f'../figures/aut_bo3_x_fig{train_size}_{test_size}_{words_num_in_par}{div}_{prep_descr}_{kinds_descr}.jpg'\n","  return path\n","\n","def get_latex_set_str(p: int, d: int):\n","  prep = preps_descr[p].partition('_')[0].upper()\n","  if d < 2:\n","    st = 'Y'\n","  elif d < 5:\n","    st = \"Y'\"\n","  else:\n","    st = 'X'\n","  return '$' + chr(92) + 'mathbb{' + st + '}_{' + prep + '}$'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9X3KQy3bJ41F"},"outputs":[],"source":["def run_pipeline():\n","  authors_ = get_existing_authors()\n","  p = Pipeline(\n","    corpus_train=train_set,\n","    corpus_test=test_set,\n","    classes_train=train_classes,\n","    classes_test=test_classes,\n","    class_names=authors_,\n","    representations=representations,\n","    models=models)\n","  p.pipelines()\n","  p.accuracy_latex_format('../results/bo3_x_table.tex', latex_set_str)\n","  p.save_img(conf_path())\n","  #expl = Explain(p)"]},{"cell_type":"markdown","source":["### Pętla klasyfikacji"],"metadata":{"id":"PT2fdGh-BEd3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YT-i_q2xOHN0"},"outputs":[],"source":["for p in preps_num:\n","  for d in div_num:\n","    number_of_dataset = str(d)+str(p) if d>0 else str(p)\n","    if d < 5:\n","      mixed = True\n","      kinds_descr = 'l'\n","    else:\n","      mixed = False\n","      kinds_descr = 'e'\n","    latex_set_str = get_latex_set_str(p,d)\n","    prep_descr = preps_descr[p]\n","    file_name_read = datasets_filepath + str(number_of_dataset)\n","    with open(file_name_read, 'rb') as f:\n","      data = pickle.load(f)\n","      head_data, train_set, train_classes, test_set, test_classes = data\n","      test_size, train_size, words_num_in_par, preprocessing_list, authors = head_data\n","    print(get_books_epoch_list())\n","    run_pipeline()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"env_python3_10","language":"python","name":"env_python3_10"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":0}